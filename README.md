# ECLU Activation Function

This project introduces the **ECLU (Euclidean Curvilinear Unit)** activation function, a novel alternative to traditional activation functions like ReLU.

## Project Structure

- `eclu_activation.py`: Defines the ECLU activation function.
- `maze.py`: A demonstration of the ECLU activation function in a neural network that learns to solve a spiral maze. It compares the performance of ECLU with ReLU.
- `maze_data.csv`: The dataset for the spiral maze, generated by `maze.py`.
- `eclu_paper.pdf`: A research paper detailing the ECLU activation function.

## ECLU Activation Function

The ECLU activation function is defined as:

`f(x) = max(0, e^(0.83x - 0.08) - 1)`

This function has a "noise gate" that outputs zero for small inputs, and then curves up exponentially for more significant inputs.

## Maze Demonstration

The `maze.py` script trains a simple neural network to distinguish between two intertwined spirals. It visualizes the decision boundary learned by the network in the terminal, allowing for a direct comparison between the ECLU and ReLU activation functions.

To run the demonstration:

```
python maze.py
```

The script will generate the `maze_data.csv` file, train the two neural networks, and then print the learned decision boundaries to the console.
